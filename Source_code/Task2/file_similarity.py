# -*- coding: utf-8 -*-
"""File_similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tRMfNIELxovFSaCWaeEwZp-V_21pvYJ_
"""

!pip install PyPDF2

!pip install python-docx

!pip install contractions

!pip install unidecode

import os
import PyPDF2
import pandas as pd
import docx
import re
import nltk
import contractions
import unidecode
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, silhouette_score
from sklearn.cluster import DBSCAN
from collections import Counter
from sklearn.metrics import silhouette_score
from sklearn.svm import LinearSVC
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
import warnings
warnings.filterwarnings('ignore')

#function to store PDF data
def store_pdf_data(file_path):
  pdf_reader = PyPDF2.PdfReader(file)
  content = ""
  for page_number in range(len(pdf_reader.pages)):
    content += pdf_reader.pages[page_number].extract_text()
  return content

#function to store DOCX data
def store_doc_data(file_path):
  doc = docx.Document(file_path)
  content = ""
  for paragraph in doc.paragraphs:
    content += paragraph.text
  return content

# storing all the data extracted from files into a dataframe
path = "/Data"
whole_content=[]
for filename in os.listdir(path):
  row_data = {}
  file_path = os.path.join(path, filename)
  row_data["file_name"]=filename
  if filename.endswith(".pdf"):
    with open(file_path, "rb") as file:
      df_content = store_pdf_data(file_path)
    row_data["content"]=df_content
    row_data["file_type"]="PDF"
    row_data["label"]= filename.split('_')[0]
  elif filename.endswith(".txt"):
    with open(file_path, "r") as file:
      df_content = file.read()
    row_data["content"]=df_content
    row_data["file_type"]="TXT"
    row_data["label"]= filename.split('_')[0]
  elif filename.endswith(".docx"):
    df_content = store_doc_data(file_path)
    row_data["content"]=df_content
    row_data["file_type"]="DOCX"
    row_data["label"]= filename.split('_')[0]
  whole_content.append(row_data)

df = pd.DataFrame(whole_content)

"""Text cleaning"""

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# function to perform text cleaning
def clean_text(text):

  #removing contractions
  text = contractions.fix(text)

  #making the text to lowercase
  text = text.lower()

  #removing non-alphabetical charecters
  text = re.sub(r'[^a-zA-Z\s]', '', text)

  #decoding the encoded data
  text = unidecode.unidecode(text)

  #performing tokenization
  tokens = nltk.word_tokenize(text)

  #removing stopwords
  stop_words = set(stopwords.words('english'))
  tokens = [word for word in tokens if word not in stop_words]

  #performing lemmatization
  lemmatizer = WordNetLemmatizer()
  tokens = [lemmatizer.lemmatize(word) for word in tokens]

  return tokens

df['cleaned_content'] = df['content'].apply(clean_text)

#encoding the labels to numerical classification
label_encoding = {
    'education': 0,
    'health': 1,
    'entertainment': 2,
}
df['encoding'] = df['label'].map(label_encoding)
X=df['cleaned_content']
y=df['encoding']

X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.8,random_state = 5778)

"""## Vectorization"""

vector_size = 100
window = 5
min_count = 3
workers = 4

#considering word2vec model to create word embeddings
model = Word2Vec(sentences=X_train, vector_size=vector_size, window=window, min_count=min_count, workers=workers)

model.save("word2vec.model")

#function creating sentence vectors using the word embeddings
def sentence_vector(words, model):
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    if not word_vectors:
        return np.zeros(model.vector_size)
    return np.mean(word_vectors, axis=0)

train_sentence_vectors = [sentence_vector(words, model) for words in X_train]

test_sentence_vectors = [sentence_vector(words, model) for words in X_test]

"""##Clustering
In order to segregate the files based on the context and metadata, we should determine the optimal 'k'

"""

from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Elbow Method
inertia = []
K = range(1, 11)

for k in K:
  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(train_sentence_vectors)
  inertia.append(kmeans.inertia_)

# Plot
plt.figure(figsize=(8, 4))
plt.plot(K, inertia, 'bo-', markersize=8)
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal k')
plt.show()

# Silhouette Method
silhouette_scores = []
K = range(2, 11)

for k in K:
  kmeans = KMeans(n_clusters=k, random_state=42)
  cluster_labels = kmeans.fit_predict(train_sentence_vectors)
  silhouette_avg = silhouette_score(train_sentence_vectors, cluster_labels)
  silhouette_scores.append(silhouette_avg)

# Plot
plt.figure(figsize=(8, 4))
plt.plot(K, silhouette_scores, 'bo-', markersize=8)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method For Optimal k')
plt.show()

"""



*  Here, the best k based on Elbow method is 3
*  Here, the best k based on Silhouette score is 2

"""

#performing kmeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(train_sentence_vectors)
labels = kmeans.labels_

test_cluster_labels = kmeans.predict(test_sentence_vectors)

test_cluster_labels

all_cluster_labels = np.concatenate([labels, test_cluster_labels])
cluster_points = {label: [] for label in set(all_cluster_labels)}
for i, label in enumerate(all_cluster_labels):
    cluster_points[label].append(i)

for cluster_label, points in cluster_points.items():
    print(f'Cluster {cluster_label}:')
    for point in points:
        print(f'- {os.listdir(path)[point]}')

all_cluster_labels = np.concatenate([labels, test_cluster_labels])
cluster_counts = {label: 0 for label in set(all_cluster_labels)}
for label in all_cluster_labels:
    cluster_counts[label] += 1

for cluster_label, count in cluster_counts.items():
    print(f'Cluster {cluster_label}: {count} points')

#Based on the Silhouette score, we can say that the clustering is done properly and not exactly since it is graeter than 0
silhouette = silhouette_score(train_sentence_vectors, labels)
print("Silhouette Score:", silhouette)

"""## Classification
If the labels of the datapoints are given, we perfrom classification as it is supervised machine learning algorithm.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

clf_log = LogisticRegression()
clf_dt = DecisionTreeClassifier()
clf_rf = RandomForestClassifier()

# A family of models are considered to perform the classification in order to acquire the best performing algorithm.
models = {
    'Logistic Regression': clf_log,
    'Decision Tree' : clf_dt,
    'Random Forest': clf_rf,
}

# model evaluations are perfromed respectively and the results are compared
def evaluate_model(model, X_train, X_test, y_train, y_test):
  model.fit(train_sentence_vectors, y_train)
  y_pred_train = model.predict(train_sentence_vectors)
  y_pred_test = model.predict(test_sentence_vectors)

  metrics = {
      'Test Accuracy': accuracy_score(y_test, y_pred_test),
      'Test Precision': precision_score(y_test, y_pred_test, average='weighted'),
      'Test Recall': recall_score(y_test, y_pred_test, average='weighted'),
      'Test F1 Score': f1_score(y_test, y_pred_test, average='weighted')
    }
  return metrics

results = {}

for model_name, model in models.items():
  metrics = evaluate_model(model, train_sentence_vectors, test_sentence_vectors, y_train, y_test)
  results[model_name] = metrics

results_df = pd.DataFrame(results).T
results_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
results_df['Algorithm'] = results_df.index
results_df = results_df.reset_index(drop=True)

print(results_df)

results_df

results_df.set_index('Algorithm', inplace=True)

ax = results_df.plot(kind='bar', figsize=(12, 6))

for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() * 1.005, p.get_height() * 1.005))

plt.title('Comparison of Classification Algorithms')
plt.ylabel('Score')
plt.xlabel('Metric')
plt.ylim(0, 1)
plt.legend(title='Algorithm')
plt.show()

#Therefore, we can say that Random Forest classifier works best among all of them.