# -*- coding: utf-8 -*-
"""Data_Engineer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kPQwcZO4wLcwA0-7uoYDVvh_BhQuyb-A
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
from tqdm import tqdm

book_titles = []
book_links = []
pages = []
authors = []
whole_data = []
data_points = 0

for page in tqdm(range(1, 2000)):
    url = "https://www.goodreads.com/search?page=" + str(page) + "&q=war&qid=tJDoGx6tm0&search_type=books&tab=books"
    #Adding user agent since blocked
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}
    response = requests.get(url, headers=headers)
    html_content = response.content.decode()
    # using BeautifulSoup since static content
    soup = BeautifulSoup(html_content, 'html.parser')
    names = soup.find_all("a", {"class": "bookTitle"})

    for link in names:
        data_points+=1

        row_data = {}


        book_title = link.text.strip()
        row_data["title"] = book_title
        book_titles.append(book_title)

        book_link = "https://www.goodreads.com" + link['href']
        book_links.append(book_link)
        #Collecting author names for every book on the page
        author_names = link.find_next("span", itemprop="author")
        a_tags = author_names.find_all('a', class_='authorName')
        authors_list = [a_tag.span.text for a_tag in a_tags]
        row_data["authors"] = authors_list


        book_response = requests.get(book_link, headers=headers)
        book_html_content = book_response.content.decode()
        book_soup = BeautifulSoup(book_html_content, 'html.parser')

        #Collecting publication year for every book on the page
        publication_date = book_soup.find("p", {"data-testid": "publicationInfo"})
        if publication_date:
          year = int(publication_date.text.split(",")[-1].strip())
          row_data["year"] = year
        else:
          row_data["year"] = None

        #filtering by year since last five years are considered
        if(year in [2024,2023,2022,2021,2020,2019]):
          book_pages = book_soup.find_all("p", {"data-testid": "pagesFormat"})
          #Collecting # of pages and book type in every book URL
          if book_pages:
            page_count = book_pages[0].text.split()[0]
            book_type = book_pages[0].text.split(",")[-1].strip()
            row_data["page_count"] = page_count
            row_data["book_type"] = book_type
            pages.append(page_count)
          else:
            row_data["page_count"] = None
            row_data["book_type"] = None

          #Collecting ratings in every book URL
          rating = book_soup.find_all("div", {"class":"RatingStatistics__rating", "aria-hidden": "true"})
          if rating:
            rating_text = rating[0].text
            row_data["rating"] = float(rating_text)
          else:
            row_data["rating"] = None


          #Collecting ratings count in every book URL
          ratings_count = book_soup.find_all("span",{"data-testid":"ratingsCount", "aria-hidden":"true"})
          if ratings_count:
            rating_count = ratings_count[0].text.split("\xa0")[0]
            rating_count = int(rating_count.replace(',', ''))
            row_data["rating_count"] = rating_count
          else:
            row_data["rating_count"] = None


          #Collecting reviews count in every book URL
          reviews_count = book_soup.find_all("span",{"data-testid":"reviewsCount", "aria-hidden":"true", "class": "u-dot-before"})
          if reviews_count:
            review_count = reviews_count[0].text.split("\xa0")[0]
            review_count = int(review_count.replace(',', ''))
            row_data["review_count"] = review_count
          else:
            row_data["review_count"] = None



          whole_data.append(row_data)

df_whole_data= pd.DataFrame(whole_data)

combined_df_tuple = df_whole_data.applymap(lambda x: tuple(x) if isinstance(x, list) else x)
unique_rows_df = combined_df_tuple.drop_duplicates(keep='last')

unique_rows_df.to_csv("data_final.csv", index=False)