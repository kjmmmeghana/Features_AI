# -*- coding: utf-8 -*-
"""Task3-RAG-FAQ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CFksXEDDEW7ANJjMC126p3BHu2xuamen
"""

pip install -q datasets sentence-transformers faiss-cpu accelerate

pip install -q bitsandbytes

pip install pypdf

pip install -q langchain_community

pip install -U langchain-text-splitters

pip install langchain

pip install nltk rouge-score

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from torch import cuda
from transformers import StoppingCriteria, StoppingCriteriaList
import transformers
from langchain_community.llms import HuggingFacePipeline
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import ConversationalRetrievalChain, LLMChain
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import nltk
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu
from statistics import mean

nltk.download('punkt')

#using opensource Llama3
model_id = "nvidia/Llama3-ChatQA-1.5-8B"

# use quantization to lower GPU usage (loading only quantized version)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)

#getting model from hugging face
hf_auth = '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'

tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=hf_auth)

#initializing model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=bnb_config
)


terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

# seeting to evaluation mode
model.eval()

# reading the PDF
loader = PyPDFLoader("/Data_pdf.pdf")
pages = loader.load_and_split()

#to understand the infromation of PDF
model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": "cuda"}

# loading from huggingface
embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)

#to store vectors formed from the pdf word embeddings
vectorstore = FAISS.from_documents(pages, embeddings)

# examples for end of prompt
stop_list = ['\nHuman:', '\n```\n']
stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]
stop_token_ids

# to use on gpus
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]
stop_token_ids

class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_ids in stop_token_ids:
            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():
                return True
        return False

# give stop tokens to generatetext
stopping_criteria = StoppingCriteriaList([StopOnTokens()])

generate_text = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    return_full_text=True,  # langchain expects the full text
    task='text-generation',
    # we pass model parameters here too
    stopping_criteria=stopping_criteria,  # without this model rambles during chat
    temperature=0.4,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max
    max_new_tokens=250,  # max number of tokens to generate in the output
    repetition_penalty=1.1  # without this output begins repeating
)

# create an instance of a pipeline using the Hugging Face Transformers library
llm = HuggingFacePipeline(pipeline=generate_text)

# creating prompt template
template = """
<<SYS>>
You are an assistant for answering questions.
You are given the extracted parts of a long document and a question. Provide a conversational answer.
If you don't know the answer, just say "I do not know." Don't make up an answer.
<</SYS>>
[INST]
Question: {question}
[/INST]
"""
prompt = ChatPromptTemplate.from_template(template)

question_generator_chain = LLMChain(llm=llm, prompt=prompt)

chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())

# for answer extraction
def extract_helpful_answer(response):
    marker = "Helpful Answer:"
    marker_index = response.find(marker)
    if marker_index != -1:
        answer_start = marker_index + len(marker)
        answer = response[answer_start:].strip()
        return answer
    else:
        return "I do not know."

# generating response from LLM
def answer_rag(query):
  result = chain({"question": query, "chat_history": chat_history})
  llm_response = result['answer']
  extracted_answer = extract_helpful_answer(llm_response)
  return extracted_answer

questions = []
with open('questions.txt', 'r') as file:
    for line in tqdm(file):
        questions.append(line.strip())

questions

answers = []
for question in tqdm(questions):
  answers.append(answer_rag(question))

gold_answers = []
with open('gold_answers.txt', 'r') as file:
    for line in file:
        gold_answers.append(line.strip())

# claculation rouge scores
rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

rouge_scores = []
for ref, gen in zip(gold_answers, answers):
    scores = rouge.score(ref, gen)
    rouge_scores.append(scores)

# calculating bleu scores
bleu_scores = []
for ref, gen in zip(gold_answers, answers):
    ref_tokens = nltk.word_tokenize(ref)
    gen_tokens = nltk.word_tokenize(gen)
    score = sentence_bleu([ref_tokens], gen_tokens)
    bleu_scores.append(score)

# rouge and bleu scores for individual question
rouge1_precisions, rouge1_recalls, rouge1_fmeasures = [], [], []
rouge2_precisions, rouge2_recalls, rouge2_fmeasures = [], [], []
rougeL_precisions, rougeL_recalls, rougeL_fmeasures = [], [], []

# Extract and print individual scores
for i, (rouge_score, bleu_score) in enumerate(zip(rouge_scores, bleu_scores)):
    rouge1_precisions.append(rouge_score['rouge1'].precision)
    rouge1_recalls.append(rouge_score['rouge1'].recall)
    rouge1_fmeasures.append(rouge_score['rouge1'].fmeasure)

    rouge2_precisions.append(rouge_score['rouge2'].precision)
    rouge2_recalls.append(rouge_score['rouge2'].recall)
    rouge2_fmeasures.append(rouge_score['rouge2'].fmeasure)

    rougeL_precisions.append(rouge_score['rougeL'].precision)
    rougeL_recalls.append(rouge_score['rougeL'].recall)
    rougeL_fmeasures.append(rouge_score['rougeL'].fmeasure)

    print(f"FAQ {i + 1}:")
    print(f"  ROUGE-1: P={rouge_score['rouge1'].precision:.4f}, R={rouge_score['rouge1'].recall:.4f}, F={rouge_score['rouge1'].fmeasure:.4f}")
    print(f"  ROUGE-2: P={rouge_score['rouge2'].precision:.4f}, R={rouge_score['rouge2'].recall:.4f}, F={rouge_score['rouge2'].fmeasure:.4f}")
    print(f"  ROUGE-L: P={rouge_score['rougeL'].precision:.4f}, R={rouge_score['rougeL'].recall:.4f}, F={rouge_score['rougeL'].fmeasure:.4f}")
    print(f"  BLEU: {bleu_score:.4f}")

# Print overall scores by calculating the averages
avg_rouge1_precision = mean(rouge1_precisions)
avg_rouge1_recall = mean(rouge1_recalls)
avg_rouge1_fmeasure = mean(rouge1_fmeasures)

avg_rouge2_precision = mean(rouge2_precisions)
avg_rouge2_recall = mean(rouge2_recalls)
avg_rouge2_fmeasure = mean(rouge2_fmeasures)

avg_rougeL_precision = mean(rougeL_precisions)
avg_rougeL_recall = mean(rougeL_recalls)
avg_rougeL_fmeasure = mean(rougeL_fmeasures)

avg_bleu = mean(bleu_scores)
print("\nOverall Scores:")
print(f"  ROUGE-1: Precision={avg_rouge1_precision:.4f}, Recall={avg_rouge1_recall:.4f}, F1-Score={avg_rouge1_fmeasure:.4f}")
print(f"  ROUGE-2: Precision={avg_rouge2_precision:.4f}, Recall={avg_rouge2_recall:.4f}, F1-Score={avg_rouge2_fmeasure:.4f}")
print(f"  ROUGE-L: Precision={avg_rougeL_precision:.4f}, Recall={avg_rougeL_recall:.4f}, F1-Score={avg_rougeL_fmeasure:.4f}")
print(f"  BLEU: {avg_bleu:.4f}")

